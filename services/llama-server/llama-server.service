[Unit]
Description=Llama.cpp Inference Server
Documentation=https://github.com/ggerganov/llama.cpp
After=network.target
Wants=network-online.target

[Service]
Type=simple
User=llama
Group=llama
WorkingDirectory=/var/lib/llama-cpp/run

# Environment
Environment="LD_LIBRARY_PATH=/usr/local/lib:/usr/local/cuda/lib64:/usr/lib/aarch64-linux-gnu/tegra"
Environment="CUDA_VISIBLE_DEVICES=0"
EnvironmentFile=/usr/local/etc/llama-cpp/llama-server.conf

# Single line execution command (with optimal DAWN settings)
ExecStart=/usr/local/bin/llama-server -m ${MODEL} --gpu-layers ${GPU_LAYERS} -c ${CONTEXT_SIZE} -b ${BATCH_SIZE} -ub ${UNBATCH_SIZE} -t ${THREADS} -ctk ${CACHE_TYPE_K} -ctv ${CACHE_TYPE_V} --flash-attn on --temp ${TEMPERATURE} --top-p ${TOP_P} --top-k ${TOP_K} --repeat-penalty ${REPEAT_PENALTY} --min-p ${MIN_P} --jinja --chat-template-file ${TEMPLATE} --host ${HOST} --port ${PORT}

# Logging
StandardOutput=append:/var/log/llama-cpp/server.log
StandardError=append:/var/log/llama-cpp/error.log

# Restart policy
Restart=on-failure
RestartSec=10s
StartLimitInterval=5min
StartLimitBurst=3

[Install]
WantedBy=multi-user.target
