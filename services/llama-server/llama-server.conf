# Llama.cpp Server Configuration for DAWN
# Updated: January 2025
#
# See README.md for full documentation

# ==============================================================================
# MODEL SELECTION - Choose ONE preset by uncommenting MODEL and REASONING_FORMAT
# ==============================================================================

# --- PRESET A: Standard Instruct (default) ---
# Fast responses, no visible reasoning. Best for general use.
MODEL="/var/lib/llama-cpp/models/Qwen3-4B-Instruct-2507-Q4_K_M.gguf"
TEMPLATE="/var/lib/llama-cpp/templates/qwen3_chatml.jinja"
REASONING_FORMAT=none

# --- PRESET B: Extended Thinking ---
# Shows AI reasoning process. Slower but transparent.
# Uncomment these 2 lines and comment out Preset A above:
#MODEL="/var/lib/llama-cpp/models/Qwen3-4B-Thinking-2507-Q4_K_M.gguf"
#TEMPLATE="/var/lib/llama-cpp/templates/qwen3_official.jinja"
#REASONING_FORMAT=deepseek

# --- PRESET C: Higher Quality (8B) ---
# Better quality but slower (~4 tok/s).
#MODEL="/var/lib/llama-cpp/models/Qwen3-8B-Q4_K_M.gguf"
#TEMPLATE="/var/lib/llama-cpp/templates/qwen3_chatml.jinja"
#REASONING_FORMAT=none

# ==============================================================================
# After changing presets, restart the service:
#   sudo systemctl restart llama-server
#
# For Thinking mode, also add to dawn.toml:
#   [llm.thinking]
#   mode = "enabled"
#   budget_tokens = 5000
# ==============================================================================

# Environment variables (Jetson-specific CUDA settings)
GGML_CUDA_NO_VMM=1
GGML_CUDA_FORCE_MMQ=1

# Server options
PORT=8080
HOST=127.0.0.1
GPU_LAYERS=99

# CRITICAL PARAMETERS (affect quality - do not change without testing)
# Batch 768 achieves 84.8% quality (vs 18% at 256, 56% at 512)
CONTEXT_SIZE=8192
BATCH_SIZE=768
UNBATCH_SIZE=768

# Qwen's Official Recommended Parameters
THREADS=4
CACHE_TYPE_K=q8_0
CACHE_TYPE_V=q8_0
TEMPERATURE=0.7
TOP_P=0.8
TOP_K=20
MIN_P=0
REPEAT_PENALTY=1.1

# Speculative decoding - DO NOT USE FOR DAWN
# Testing showed 40% SLOWER for DAWN's use case (long prompts, short responses)
# DRAFT_MODEL="/var/lib/llama-cpp/models/Qwen3-0.6B-Q8_0.gguf"
