# Llama.cpp Server Configuration for DAWN
# Updated: December 2024
#
# OPTIMAL: Qwen3-4B-Instruct-2507-Q4_K_M
#   - Quality: 84.8% (B grade) with Qwen's official parameters
#   - Speed: 15.2 tok/s
#   - TTFT: ~120ms (excellent for streaming TTS)
#   - Memory: ~5GB total (leaves ~11GB free on 16GB Jetson)
#
# See llm_testing/docs/MODEL_TEST_ANALYSIS.md for full testing details

# Environment variables
GGML_CUDA_NO_VMM=1
GGML_CUDA_FORCE_MMQ=1

# Model and template paths
# RECOMMENDED: 4B model achieves 84.8% quality @ 15.2 tok/s
MODEL="/var/lib/llama-cpp/models/Qwen3-4B-Instruct-2507-Q4_K_M.gguf"
# ALTERNATIVE: 8B model - higher quality but slower (~4 tok/s)
#MODEL="/var/lib/llama-cpp/models/Qwen3-8B-Q4_K_M.gguf"
TEMPLATE="/var/lib/llama-cpp/templates/qwen3_chatml.jinja"

# Speculative decoding - DO NOT USE FOR DAWN
# Testing showed speculative decoding is 40% SLOWER for DAWN's use case:
#   - Long system prompt (90 tokens) + short responses (30-50 tokens)
#   - Overhead of running both models exceeds benefit
#   - NO spec: 15.2 tok/s average
#   - WITH spec: 9.4 tok/s average
# Only beneficial for: short prompts, long responses, exact prompt caching
# DRAFT_MODEL="/var/lib/llama-cpp/models/Qwen3-0.6B-Q8_0.gguf"

# Server options
PORT=8080
HOST=127.0.0.1
GPU_LAYERS=99

# CRITICAL PARAMETERS (affect quality)
# Batch 768 is optimal - achieves 84.8% quality
# Batch size discovery: 18% @ 256 -> 56% @ 512 -> 84.8% @ 768
# Context must be large enough for: system prompt (~800 tokens) + device list (~300)
#   + command instructions (~200) + conversation history (~500) + response (~200)
# Total needed: ~2000+ tokens minimum, 8192 provides headroom for longer conversations
CONTEXT_SIZE=8192
BATCH_SIZE=768
UNBATCH_SIZE=768

# Qwen's Official Recommended Parameters
# These achieve best quality (84.8%) based on Qwen team recommendations
# Note: Testing showed temp/top_p/top_k have minimal effect vs batch size
THREADS=4
CACHE_TYPE_K=q8_0
CACHE_TYPE_V=q8_0
TEMPERATURE=0.7
TOP_P=0.8
TOP_K=20
MIN_P=0
REPEAT_PENALTY=1.1
