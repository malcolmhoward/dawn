# Llama.cpp Server Configuration
# OPTIMAL SETTINGS for DAWN (81.9% quality, B grade)
# Tested 31+ configurations - these settings achieve best performance

# Environment variables
GGML_CUDA_NO_VMM=1
GGML_CUDA_FORCE_MMQ=1

# Model and template paths
# CRITICAL: Use Q4 model (NOT Q6) - Q6 produces <think> loops
MODEL="/var/lib/llama-cpp/models/Qwen3-4B-Instruct-2507-Q4_K_M.gguf"
TEMPLATE="/var/lib/llama-cpp/templates/qwen3_chatml.jinja"

# Server options
PORT=8080
HOST=127.0.0.1
GPU_LAYERS=99

# CRITICAL PARAMETERS (affect quality)
# Batch 768 achieves 81.9% quality (vs 18% at 256, 56% at 512)
# TTFT: 116-138ms (excellent for streaming TTS)
CONTEXT_SIZE=1024
BATCH_SIZE=768
UNBATCH_SIZE=768

# Standard parameters (tested - no effect on quality)
THREADS=4
CACHE_TYPE_K=q8_0
CACHE_TYPE_V=q8_0
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=40
MIN_P=0
REPEAT_PENALTY=1.1
