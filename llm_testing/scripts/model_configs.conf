# Model-Specific Configuration for llama-server
# Updated: December 2024
#
# Each model has optimized settings based on:
# - HuggingFace model card recommendations
# - llama.cpp best practices
# - Empirical testing results (see llm_testing/docs/MODEL_TEST_ANALYSIS.md)
#
# RECOMMENDED: Qwen3-4B-Instruct-2507-Q4_K_M
#   - Quality: 84.8% (B grade)
#   - Speed: 15.2 tok/s
#   - TTFT: ~120ms
#
# SPECULATIVE DECODING: DO NOT USE FOR DAWN
#   - Testing showed 40% SLOWER with DAWN's prompt structure
#   - Long system prompt (90 tokens) + short responses = worst case for spec decoding
#   - See MODEL_TEST_ANALYSIS.md for full analysis

# Format: MODEL_NAME|gpu_layers|context|batch|ubatch|threads|temp|top_p|top_k|repeat_penalty|extra_flags

# =============================================================================
# QWEN MODELS
# =============================================================================

# Qwen3-4B-Instruct-2507-Q4_K_M
# - HuggingFace: Qwen/Qwen3-4B-Instruct (trained with temp 0.7, top-p 0.8)
# - ⭐ OPTIMAL: Batch 768 achieves 84.8% quality (B grade) @ 15.3 tok/s
# - Qwen recommended: temp=0.7, top_p=0.8, top_k=20, min_p=0.0
# - Context 8192: DAWN's full system prompt + device list + conversation = ~2000+ tokens
Qwen3-4B-Instruct-2507-Q4_K_M.gguf|99|8192|768|768|4|0.7|0.8|20|1.1|--flash-attn on --min-p 0

# Unsloth Dynamic 2.0 GGUFs - Qwen3-4B-Instruct-2507
# - Uses Qwen's exact recommended params: temp=0.7, top_p=0.8, top_k=20, min_p=0.0
# - 262K context requires batch 512 on Jetson (768 OOMs)
Qwen3-4B-Instruct-2507-UD-Q3_K_XL.gguf|99|1024|384|384|4|0.7|0.8|20|1.1|--flash-attn on --min-p 0
Qwen3-4B-Instruct-2507-UD-Q4_K_XL.gguf|99|1024|512|512|4|0.7|0.8|20|1.1|--flash-attn on --min-p 0
Qwen3-4B-Instruct-2507-UD-Q5_K_XL.gguf|99|1024|512|512|4|0.7|0.8|20|1.1|--flash-attn on --min-p 0

# Qwen3-8B-Q4_K_M
# - Larger 8B model - higher quality but slower (~4 tok/s)
# - Use when quality is more important than speed
# - May need batch 512 to avoid OOM on 16GB Jetson with 8192 context
Qwen3-8B-Q4_K_M.gguf|99|8192|512|512|4|0.7|0.8|20|1.1|--flash-attn on --min-p 0

# Qwen3-0.6B-Q8_0 (Draft model for speculative decoding)
# - Small model for speculative decoding experiments
# - NOTE: Spec decoding is NOT recommended for DAWN (40% slower)
# - Only useful for: short prompts, long responses, exact prompt caching
Qwen3-0.6B-Q8_0.gguf|99|1024|512|512|4|0.7|0.8|20|1.1|--flash-attn on --min-p 0

# Qwen_Qwen3-4B-Q6_K_L
# - Same architecture as Q4, higher quantization
# - Known issue: <think> loops with some prompts (template issue)
# - Not recommended over Q4_K_M
Qwen_Qwen3-4B-Q6_K_L.gguf|99|1024|768|768|4|0.7|0.9|40|1.1|--flash-attn on

# Qwen2.5-7B-Instruct-Q4_K_M
# - HuggingFace: Qwen/Qwen2.5-7B-Instruct (trained with temp 0.7, top-p 0.8)
# - RETEST: Batch 768 for fair comparison (may OOM - will reduce if needed)
Qwen2.5-7B-Instruct-Q4_K_M.gguf|99|1024|768|768|4|0.7|0.9|40|1.1|--flash-attn on

# =============================================================================
# LLAMA MODELS
# =============================================================================

# Llama-3.2-3B-Instruct-Q4_K_M
# - HuggingFace: meta-llama/Llama-3.2-3B-Instruct
# - Recommended: temp 0.6, top-p 0.9 (from Meta's model card)
# - RETEST: Batch 768 for fair comparison (previous tests showed gibberish)
Llama-3.2-3B-Instruct-Q4_K_M.gguf|99|1024|768|768|4|0.5|0.85|20|1.3|--flash-attn on

# =============================================================================
# PHI MODELS
# =============================================================================

# Phi-3-mini-4k-instruct-q4
# - HuggingFace: microsoft/Phi-3-mini-4k-instruct
# - Recommended: temp 0.7, but testing showed needs conservative
# - RETEST: Batch 768 for fair comparison (previous tests showed blank output)
Phi-3-mini-4k-instruct-q4.gguf|99|1024|768|768|4|0.5|0.85|20|1.3|--flash-attn on

# =============================================================================
# NOTES
# =============================================================================
#
# GPU Layers (--gpu-layers):
# - 99 = "offload all layers" (llama.cpp auto-detects max)
# - Jetson has unified memory, so no CPU/GPU distinction
# - Only limited by compute buffer size, not memory capacity
#
# Context Size (-c):
# - 1024 is optimal for DAWN's use case (rarely exceeds this)
# - Larger contexts = larger compute buffers = risk OOM
# - Smaller contexts = faster but may truncate complex prompts
#
# Batch Size (-b, -ub):
# - CRITICAL FOR QUALITY: Batch 768+ required for good quality on Qwen models
# - Qwen3-4B Q4: 18% @ batch 256 → 82.9% @ batch 768 (massive improvement!)
# - Testing showed batch size is THE parameter that matters (not temp/top-k/top-p)
# - 768 is now standard for all models for fair comparison
# - 7B models may OOM at 768 (will reduce to 512 or 384 if needed)
# - ubatch (microbatch) should match batch for simplicity
#
# Temperature:
# - NOTE: Testing showed NO EFFECT on quality (tested 0.5, 0.7, 0.9, 1.0 all = 82.9%)
# - Only batch/context matter for quality
# - Kept at 0.7 for Qwen (standard), 0.5 for Llama/Phi (conservative)
#
# Top-P (nucleus sampling):
# - NOTE: Testing showed NO EFFECT on quality (tested 0.8, 0.85, 0.9, 0.95 all = 82.9%)
# - Kept at standard values for consistency
#
# Top-K (token selection pool):
# - NOTE: Testing showed NO EFFECT on quality (tested 20, 40, 60, 100 all = 82.9%)
# - Kept at standard values for consistency
#
# Repeat Penalty:
# - NOTE: Testing showed NO EFFECT on quality (tested 1.0, 1.1, 1.2, 1.3 all = 82.9%)
# - Kept at standard values for consistency
#
# Flash Attention:
# - Always enabled on supported hardware (Jetson Orin)
# - Faster and more memory efficient
#
