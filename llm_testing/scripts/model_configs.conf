# Model-Specific Configuration for llama-server
# Each model has optimized settings based on:
# - HuggingFace model card recommendations
# - llama.cpp best practices
# - Empirical testing results

# Format: MODEL_NAME|gpu_layers|context|batch|ubatch|threads|temp|top_p|top_k|repeat_penalty|extra_flags

# =============================================================================
# QWEN MODELS
# =============================================================================

# Qwen3-4B-Instruct-2507-Q4_K_M
# - HuggingFace: Qwen/Qwen3-4B-Instruct (trained with temp 0.7, top-p 0.8)
# - ⭐ OPTIMAL: Batch 768 achieves 82.9% quality (B grade) @ 14.9 tok/s
# - Tested 19 configurations - batch size is the only parameter that matters
Qwen3-4B-Instruct-2507-Q4_K_M.gguf|99|1024|768|768|4|0.7|0.9|40|1.1|--flash-attn

# Qwen_Qwen3-4B-Q6_K_L
# - Same architecture as Q4, higher quantization
# - RETEST: Batch 768 for fair comparison with Q4
Qwen_Qwen3-4B-Q6_K_L.gguf|99|1024|768|768|4|0.7|0.9|40|1.1|--flash-attn

# Qwen2.5-7B-Instruct-Q4_K_M
# - HuggingFace: Qwen/Qwen2.5-7B-Instruct (trained with temp 0.7, top-p 0.8)
# - RETEST: Batch 768 for fair comparison (may OOM - will reduce if needed)
Qwen2.5-7B-Instruct-Q4_K_M.gguf|99|1024|768|768|4|0.7|0.9|40|1.1|--flash-attn

# =============================================================================
# LLAMA MODELS
# =============================================================================

# Llama-3.2-3B-Instruct-Q4_K_M
# - HuggingFace: meta-llama/Llama-3.2-3B-Instruct
# - Recommended: temp 0.6, top-p 0.9 (from Meta's model card)
# - RETEST: Batch 768 for fair comparison (previous tests showed gibberish)
Llama-3.2-3B-Instruct-Q4_K_M.gguf|99|1024|768|768|4|0.5|0.85|20|1.3|--flash-attn

# =============================================================================
# PHI MODELS
# =============================================================================

# Phi-3-mini-4k-instruct-q4
# - HuggingFace: microsoft/Phi-3-mini-4k-instruct
# - Recommended: temp 0.7, but testing showed needs conservative
# - RETEST: Batch 768 for fair comparison (previous tests showed blank output)
Phi-3-mini-4k-instruct-q4.gguf|99|1024|768|768|4|0.5|0.85|20|1.3|--flash-attn

# =============================================================================
# NOTES
# =============================================================================
#
# GPU Layers (--gpu-layers):
# - 99 = "offload all layers" (llama.cpp auto-detects max)
# - Jetson has unified memory, so no CPU/GPU distinction
# - Only limited by compute buffer size, not memory capacity
#
# Context Size (-c):
# - 1024 is optimal for DAWN's use case (rarely exceeds this)
# - Larger contexts = larger compute buffers = risk OOM
# - Smaller contexts = faster but may truncate complex prompts
#
# Batch Size (-b, -ub):
# - CRITICAL FOR QUALITY: Batch 768+ required for good quality on Qwen models
# - Qwen3-4B Q4: 18% @ batch 256 → 82.9% @ batch 768 (massive improvement!)
# - Testing showed batch size is THE parameter that matters (not temp/top-k/top-p)
# - 768 is now standard for all models for fair comparison
# - 7B models may OOM at 768 (will reduce to 512 or 384 if needed)
# - ubatch (microbatch) should match batch for simplicity
#
# Temperature:
# - NOTE: Testing showed NO EFFECT on quality (tested 0.5, 0.7, 0.9, 1.0 all = 82.9%)
# - Only batch/context matter for quality
# - Kept at 0.7 for Qwen (standard), 0.5 for Llama/Phi (conservative)
#
# Top-P (nucleus sampling):
# - NOTE: Testing showed NO EFFECT on quality (tested 0.8, 0.85, 0.9, 0.95 all = 82.9%)
# - Kept at standard values for consistency
#
# Top-K (token selection pool):
# - NOTE: Testing showed NO EFFECT on quality (tested 20, 40, 60, 100 all = 82.9%)
# - Kept at standard values for consistency
#
# Repeat Penalty:
# - NOTE: Testing showed NO EFFECT on quality (tested 1.0, 1.1, 1.2, 1.3 all = 82.9%)
# - Kept at standard values for consistency
#
# Flash Attention:
# - Always enabled on supported hardware (Jetson Orin)
# - Faster and more memory efficient
#
